{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "#text preprocessing functions:\n",
    "def text_cleaning(text):\n",
    "    #to lower case\n",
    "    text = text.lower()\n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #remove puctuations\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    #remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(df):\n",
    "    #text_cleaning\n",
    "    df['review'] = df['review'].apply(text_cleaning)\n",
    "    #tokenization\n",
    "    df['review'] = df.apply(lambda x: word_tokenize(x['review']), axis = 1)\n",
    "    #remove stopwords\n",
    "    df['review'] = df.apply(lambda x: [i for i in x['review'] if not i in stopwords], axis = 1)\n",
    "    return df\n",
    "\n",
    "def text_stemming(df):\n",
    "    df = text_preprocessing(df)\n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer(language = 'english')\n",
    "    df['review'] = df.apply(lambda x: [stemmer.stem(i) for i in x['review']], axis = 1)\n",
    "    return df\n",
    "\n",
    "def text_lemmatize(df):\n",
    "    df = text_preprocessing(df)\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['review'] = df.apply(lambda x: [lemmatizer.lemmatize(i) for i in x['review']], axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "#naive bayes classifier functions:\n",
    "def add_k_word_score(word, k, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg):\n",
    "    if word in V_dict:\n",
    "        P_word_pos = (pos_dict.get(word, 0) + k / (N_word_pos + V_size * k))\n",
    "        P_word_neg = (neg_dict.get(word, 0) + k / (N_word_neg + V_size * k))\n",
    "        return np.log(P_word_pos / P_word_neg)\n",
    "    else:\n",
    "        P_word_pos = (k / (N_word_pos + V_size * k))\n",
    "        P_word_neg = (k / (N_word_neg + V_size * k))\n",
    "        return np.log(P_word_pos / P_word_neg) \n",
    "                      \n",
    "def add_k_score(text, k, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        score += add_k_word_score(word, k, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg)\n",
    "    return(score)\n",
    "\n",
    "def add_k_smoothing_naive_bayes_classifier(train_set, test_set, k):\n",
    "    #Parameters for the classifier\n",
    "    #all unique words and its counts from all document\n",
    "    V_dict = Counter([item for sublist in train_set['review'].values.tolist() for item in sublist]) \n",
    "    #number of unique words across all document\n",
    "    V_size = len(V_dict.keys())\n",
    "    #number of document from each class\n",
    "    N_doc_pos = sum(train_set.label == 1)\n",
    "    N_doc_neg = sum(train_set.label == 0)\n",
    "    #probability of document from each class\n",
    "    P_c_pos = N_doc_pos / len(train_set)\n",
    "    P_c_neg = N_doc_neg / len(train_set)\n",
    "    #unique words and its counts from each class\n",
    "    pos_dict = Counter([item for sublist in train_set.loc[train_set.label == 1]['review'].values.tolist() for item in sublist])\n",
    "    neg_dict = Counter([item for sublist in train_set.loc[train_set.label == 0]['review'].values.tolist() for item in sublist])\n",
    "    #number of unique words from each class\n",
    "    N_word_pos = len(pos_dict.keys())\n",
    "    N_word_neg = len(neg_dict.keys())\n",
    "    \n",
    "    test_set['score'] = test_set.review.apply(lambda x: add_k_score(x, k, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg))\n",
    "    test_set['prediction'] = test_set.score.apply(lambda x: int(x > 0))\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "def interpolated_word_score(word, l, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg):\n",
    "    if word in V_dict:\n",
    "        P_word_pos = (1 - l) * (pos_dict.get(word, 0)/ N_word_pos) + l * (V_dict.get(word, 0)/ V_size)\n",
    "        P_word_neg = (1 - l) * (neg_dict.get(word, 0)/ N_word_neg) + l * (V_dict.get(word, 0)/ V_size)\n",
    "        return np.log(P_word_pos / P_word_neg)\n",
    "    else:\n",
    "        P_word_pos = N_word_pos / V_size\n",
    "        P_word_neg = N_word_neg / V_size\n",
    "        return np.log(P_word_pos / P_word_neg)\n",
    "                      \n",
    "def interpolated_score(text, l, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        score += interpolated_word_score(word, l, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg)\n",
    "    return(score)\n",
    "\n",
    "def interpolated_smoothing_naive_bayes_classifier(train_set, test_set, l):\n",
    "    #Parameters for the classifier\n",
    "    #all unique words and its counts from all document\n",
    "    V_dict = Counter([item for sublist in train_set['review'].values.tolist() for item in sublist]) \n",
    "    #number of unique words across all document\n",
    "    V_size = len(V_dict.keys())\n",
    "    #number of document from each class\n",
    "    N_doc_pos = sum(train_set.label == 1)\n",
    "    N_doc_neg = sum(train_set.label == 0)\n",
    "    #probability of document from each class\n",
    "    P_c_pos = N_doc_pos / len(train_set)\n",
    "    P_c_neg = N_doc_neg / len(train_set)\n",
    "    #unique words and its counts from each class\n",
    "    pos_dict = Counter([item for sublist in train_set.loc[train_set.label == 1]['review'].values.tolist() for item in sublist])\n",
    "    neg_dict = Counter([item for sublist in train_set.loc[train_set.label == 0]['review'].values.tolist() for item in sublist])\n",
    "    #number of unique words from each class\n",
    "    N_word_pos = len(pos_dict.keys())\n",
    "    N_word_neg = len(neg_dict.keys())\n",
    "    \n",
    "    test_set['score'] = test_set.review.apply(lambda x: interpolated_score(x, l, V_dict, V_size, pos_dict, neg_dict, N_word_pos, N_word_neg))\n",
    "    test_set['prediction'] = test_set.score.apply(lambda x: int(x > 0))\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "\n",
    "#F1 measure functions:\n",
    "def recall(df):\n",
    "    return len(df[(df['label'] == 1) & (df['prediction'] == 1)]) / len(df[(df['label'] == 1)])\n",
    "\n",
    "def precision(df):\n",
    "    return len(df[(df['label'] == 1) & (df['prediction'] == 1)]) / len(df[(df['prediction'] == 1)])\n",
    "    \n",
    "def f1score(df):\n",
    "    return (2 * recall(df) * precision(df)) / (recall(df) + precision(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #reading the csv file as datafarme object with relative path\n",
    "    fname = 'imdb_master.csv'\n",
    "    df = pd.read_csv(fname, encoding='latin1')\n",
    "\n",
    "    #exclude all entries without being labelled pos or neg\n",
    "    df = df[df['label'] != 'unsup']\n",
    "    \n",
    "    #mapping the label pos as 1 and neg as 0\n",
    "    df['label'] = df.label.map({'pos': 1, 'neg': 0})\n",
    "    \n",
    "    #the below line inspected the number of entries with each label, which is 25,000 entries for both label\n",
    "    #print(df['label'].value_counts())\n",
    "\n",
    "    #splitting the training and testing data\n",
    "    train_set = df[df['type'] == 'train']\n",
    "    test_set = df[df['type'] == 'test']\n",
    "\n",
    "    #dropping unhelpful columns\n",
    "    train_set = train_set.drop(columns= ['type', 'Unnamed: 0', 'file', 'file'])\n",
    "    test_set = test_set.drop(columns= ['type', 'Unnamed: 0', 'file', 'file'])\n",
    "\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.append('<br />')\n",
    "    \n",
    "    #preprocess both dataset without stemming or lemmatization\n",
    "    train_set_cleaned = text_preprocessing(train_set.copy())\n",
    "    test_set_cleaned = text_preprocessing(test_set.copy())\n",
    "    \n",
    "    #preprocess both dataset with stemming\n",
    "    train_set_stemming = text_stemming(train_set.copy())  \n",
    "    test_set_stemming = text_stemming(test_set.copy())\n",
    "    \n",
    "    #preprocess both dataset with stemming\n",
    "    train_set_lemmatize = text_lemmatize(train_set.copy())  \n",
    "    test_set_lemmatize = text_lemmatize(test_set.copy())\n",
    "    \n",
    "    #Without stemming or lemmatization\n",
    "    f1score_clean = []\n",
    "    #add 1 & add 10 smoothing\n",
    "    clean_result_add_1 = add_k_smoothing_naive_bayes_classifier(train_set_cleaned, test_set_cleaned, 1)\n",
    "    f1score_clean.append(f1score(clean_result_add_1))\n",
    "    clean_result_add_10 = add_k_smoothing_naive_bayes_classifier(train_set_cleaned, test_set_cleaned, 10)\n",
    "    f1score_clean.append(f1score(clean_result_add_10))\n",
    "\n",
    "    #interpolation smoothing with lambda = 0.1, 0.5, 0.9\n",
    "    clean_result_inter_1 = interpolated_smoothing_naive_bayes_classifier(train_set_cleaned, test_set_cleaned, 0.1)\n",
    "    f1score_clean.append(f1score(clean_result_inter_1))\n",
    "    clean_result_inter_2 = interpolated_smoothing_naive_bayes_classifier(train_set_cleaned, test_set_cleaned, 0.6)\n",
    "    f1score_clean.append(f1score(clean_result_inter_2))\n",
    "    clean_result_inter_3 = interpolated_smoothing_naive_bayes_classifier(train_set_cleaned, test_set_cleaned, 0.9)\n",
    "    f1score_clean.append(f1score(clean_result_inter_3))\n",
    "\n",
    "\n",
    "    #Stemming\n",
    "    f1score_stemming = []\n",
    "    #add 1 & add 10 smoothing\n",
    "    stem_result_add_1 = add_k_smoothing_naive_bayes_classifier(train_set_stemming, test_set_stemming, 1)\n",
    "    f1score_stemming.append(f1score(stem_result_add_1))\n",
    "    stem_result_add_10 = add_k_smoothing_naive_bayes_classifier(train_set_stemming, test_set_stemming, 10)\n",
    "    f1score_stemming.append(f1score(stem_result_add_10))\n",
    "\n",
    "    #interpolation smoothing with lambda = 0.1, 0.5, 0.9\n",
    "    stem_result_inter_1 = interpolated_smoothing_naive_bayes_classifier(train_set_stemming, test_set_stemming, 0.1)\n",
    "    f1score_stemming.append(f1score(stem_result_inter_1))\n",
    "    stem_result_inter_2 = interpolated_smoothing_naive_bayes_classifier(train_set_stemming, test_set_stemming, 0.6)\n",
    "    f1score_stemming.append(f1score(stem_result_inter_2))\n",
    "    stem_result_inter_3 = interpolated_smoothing_naive_bayes_classifier(train_set_stemming, test_set_stemming, 0.9)\n",
    "    f1score_stemming.append(f1score(stem_result_inter_3))\n",
    "\n",
    "\n",
    "    #Lemmatization\n",
    "    f1score_lemma = []\n",
    "    #add 1 & add 10 smoothing\n",
    "    lemma_result_add_1 = add_k_smoothing_naive_bayes_classifier(train_set_lemmatize, test_set_lemmatize, 1)\n",
    "    f1score_lemma.append(f1score(lemma_result_add_1))\n",
    "    lemma_result_add_10 = add_k_smoothing_naive_bayes_classifier(train_set_lemmatize, test_set_lemmatize, 10)\n",
    "    f1score_lemma.append(f1score(lemma_result_add_10))\n",
    "\n",
    "    #interpolation smoothing with lambda = 0.1, 0.5, 0.9\n",
    "    lemma_result_inter_1 = interpolated_smoothing_naive_bayes_classifier(train_set_lemmatize, test_set_lemmatize, 0.1)\n",
    "    f1score_lemma.append(f1score(lemma_result_inter_1))\n",
    "    lemma_result_inter_2 = interpolated_smoothing_naive_bayes_classifier(train_set_lemmatize, test_set_lemmatize, 0.6)\n",
    "    f1score_lemma.append(f1score(lemma_result_inter_2))\n",
    "    lemma_result_inter_3 = interpolated_smoothing_naive_bayes_classifier(train_set_lemmatize, test_set_lemmatize, 0.9)\n",
    "    f1score_lemma.append(f1score(lemma_result_inter_3))    \n",
    "\n",
    "    results = {'Add 1 smoothing':[f1score_clean[0], f1score_stemming[0], f1score_lemma[0]], \n",
    "               'Add 10 smoothing':[f1score_clean[1], f1score_stemming[1], f1score_lemma[1]], \n",
    "               'Interpolation lambda = 0.1': [f1score_clean[2], f1score_stemming[2], f1score_lemma[2]], \n",
    "               'Interpolation lambda = 0.6': [f1score_clean[3], f1score_stemming[3], f1score_lemma[3]], \n",
    "               'Interpolation lambda = 0.9': [f1score_clean[4], f1score_stemming[4], f1score_lemma[4]]}\n",
    "\n",
    "    results_df = pd.DataFrame(results, index = ['Without stemming or lemmatization', 'Stemming', 'Lemmatization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Add 1 smoothing</th>\n",
       "      <th>Add 10 smoothing</th>\n",
       "      <th>Interpolation lambda = 0.1</th>\n",
       "      <th>Interpolation lambda = 0.6</th>\n",
       "      <th>Interpolation lambda = 0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Without stemming or lemmatization</th>\n",
       "      <td>0.742427</td>\n",
       "      <td>0.745061</td>\n",
       "      <td>0.803281</td>\n",
       "      <td>0.809406</td>\n",
       "      <td>0.814431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stemming</th>\n",
       "      <td>0.747816</td>\n",
       "      <td>0.750650</td>\n",
       "      <td>0.799134</td>\n",
       "      <td>0.807304</td>\n",
       "      <td>0.809858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatization</th>\n",
       "      <td>0.744938</td>\n",
       "      <td>0.748013</td>\n",
       "      <td>0.802502</td>\n",
       "      <td>0.809726</td>\n",
       "      <td>0.813995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Add 1 smoothing  Add 10 smoothing  \\\n",
       "Without stemming or lemmatization         0.742427          0.745061   \n",
       "Stemming                                  0.747816          0.750650   \n",
       "Lemmatization                             0.744938          0.748013   \n",
       "\n",
       "                                   Interpolation lambda = 0.1  \\\n",
       "Without stemming or lemmatization                    0.803281   \n",
       "Stemming                                             0.799134   \n",
       "Lemmatization                                        0.802502   \n",
       "\n",
       "                                   Interpolation lambda = 0.6  \\\n",
       "Without stemming or lemmatization                    0.809406   \n",
       "Stemming                                             0.807304   \n",
       "Lemmatization                                        0.809726   \n",
       "\n",
       "                                   Interpolation lambda = 0.9  \n",
       "Without stemming or lemmatization                    0.814431  \n",
       "Stemming                                             0.809858  \n",
       "Lemmatization                                        0.813995  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
