{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import math\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(n):\n",
    "    zip_file = ZipFile('pdf_json.zip','r')\n",
    "    random_files = np.random.choice(zip_file.namelist(), n)\n",
    "    for names in zip_file.namelist():\n",
    "        if names in random_files:\n",
    "            zip_file.extract(names)\n",
    "    #Transform json files to dataframe\n",
    "    json_path = 'pdf_json/*.json' \n",
    "    files = glob(json_path)\n",
    "    data = []\n",
    "    for file in files:\n",
    "        with open(file) as pdf:\n",
    "            json_data = json.loads(pdf.read())\n",
    "            data.append(json_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    #Drop columns that will not be used \n",
    "    df = df.drop(columns = ['bib_entries', 'ref_entries', 'back_matter'])\n",
    "    with open('metadata.csv', 'rb') as file:\n",
    "        metadata = pd.read_csv(file)\n",
    "    #Drop useless columns\n",
    "    metadata = metadata.drop(columns = ['cord_uid', 'source_x','pmcid','pubmed_id', 'license','mag_id', 'who_covidence_id', 'pmc_json_files', 'arxiv_id','s2_id', 'pdf_json_files'])\n",
    "    #Joining json dataframe and metatdata.csv\n",
    "    df_merge = df.merge(metadata, how = 'left', left_on = 'paper_id', right_on = 'sha')\n",
    "    #Remane abstract column\n",
    "    df_merge = df_merge.rename(columns = {\"abstract_y\": \"abstract\"})\n",
    "    #Merge all body text into one string and store in main_content column\n",
    "    #Remove body_text afterwards\n",
    "    for i, row in df_merge.iterrows():\n",
    "        try:\n",
    "            content = \"\"\n",
    "            body_text = pd.DataFrame(df_merge.loc[i]['body_text'])\n",
    "            for text in body_text['text']:\n",
    "                content += text\n",
    "            df_merge.at[i, 'main_content'] = content\n",
    "        except IndexError:\n",
    "            df_merge.at[i, 'main_content'] = np.nan\n",
    "    df_merge = df_merge.drop(columns = ['body_text'])\n",
    "    #Set the country of first author with non-empty country as the country of the article is from\n",
    "    #Remove metatdata afterwards\n",
    "    for i, row in df_merge.iterrows():\n",
    "        try:\n",
    "            c = 0\n",
    "            country_name = df_merge.loc[i]['metadata']['authors'][c]['affiliation']['location']['country']\n",
    "            while(country_name == np.nan):\n",
    "                c += 1\n",
    "                country_name = df_merge.loc[i]['metadata']['authors'][c]['affiliation']['location']['country']\n",
    "            df_merge.at[i, 'country'] = country_name\n",
    "        except:\n",
    "            continue\n",
    "    df_merge = df_merge.drop(columns = ['metadata'])\n",
    "    #Replace null under abstract with empty string\n",
    "    df_merge['abstract'].fillna(\"\", inplace = True)\n",
    "    #Remove duplicated columns to reduce dimension\n",
    "    df_merge = df_merge.drop(columns = ['abstract_x', 'sha'])\n",
    "    df_merge = df_merge[~df_merge['title'].isna()]\n",
    "    df_merge = df_merge.reset_index(drop = True)\n",
    "    #Save cleaned dataframe\n",
    "    df_merge.to_pickle('merged_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process1(text):\n",
    "    #lowercase all characters\n",
    "    text = text.lower()\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[%s]' % re.escape(punc), ' ', text)\n",
    "    #remove unicode text\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove the numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    #remove double space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def text_process2(text):\n",
    "    #lowercase all characters\n",
    "    text = text.lower()\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    #remove unicode text\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove the numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    #remove double space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit the two lines below according to the comment to retrieve a new set of articles\n",
    "\n",
    "retrieve_new = False #Set to True to retrieve a new set of articles \n",
    "n = 6000 #Set this value to number of articles to be retrieved\n",
    "\n",
    "if(retrieve_new):\n",
    "    retrieve(n)\n",
    "\n",
    "#Load clean dataframe\n",
    "df_merge = pd.read_pickle('merged_df')\n",
    "\n",
    "#Check\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###EDA before training model\n",
    "   \n",
    "#Plot top 10 count of articles by country\n",
    "df_merge['country'].value_counts().head(10).plot(kind = 'bar', figsize = (12, 7), title = 'Number of articles by country')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.savefig('Number of articles by country.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number of articles published after 2003 in the sample\n",
    "\n",
    "df_plot = df_merge[[\"publish_time\"]].copy()\n",
    "df_plot['publish_time'] = pd.to_datetime(df_plot['publish_time'])\n",
    "minimum = pd.to_datetime(datetime.date(2003, 1, 1))\n",
    "df = df_plot[df_plot['publish_time'] >= minimum]\n",
    "p = df['publish_time'].groupby(df['publish_time'].dt.to_period(\"M\")).agg('count')\n",
    "#Plot top 10 count of articles by country\n",
    "p.plot(kind = 'line', figsize = (12, 7), title = 'Number of articles published after 2003')\n",
    "plt.xlabel('Year of publication')\n",
    "plt.savefig('Number of articles published after 2003.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Text preprocessing and normalization\n",
    "\n",
    "abstract_tokenized = []\n",
    "main_content_tokenized = []\n",
    "sentence_list = []\n",
    "\n",
    "#Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"et\")\n",
    "stop_words.add(\"al\")\n",
    "stop_words.add(\"also\")\n",
    "stop_words.add(\"may\")\n",
    "\n",
    "punc = string.punctuation\n",
    "punc = punc.replace(\".\", \"\")\n",
    "\n",
    "#Read abstract\n",
    "for i, row in df_merge.iterrows():\n",
    "    abstract_sent = []\n",
    "    try:\n",
    "        text = df_merge.at[i, 'abstract']\n",
    "        splitted_text = text.split(sep = '.')     \n",
    "        for sentence in splitted_text:\n",
    "            if(sentence != ''):\n",
    "                sentence = text_process1(sentence)\n",
    "                #tokenize and lemmatize the sentence\n",
    "                word_tokens = word_tokenize(sentence)\n",
    "                token_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                lemmatized_text = []\n",
    "                for word in token_text:\n",
    "                    lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "                abstract_sent.append(lemmatized_text)\n",
    "                sentence_list.append(lemmatized_text)\n",
    "        abstract_tokenized.append(abstract_sent)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "#Read main content\n",
    "for i, row in df_merge.iterrows():\n",
    "    body_sent = []\n",
    "    try:\n",
    "        text = df_merge.at[i, 'main_content']\n",
    "        splitted_text = text.split(sep = '.')\n",
    "        for sentence in splitted_text:\n",
    "            if(sentence != ''):\n",
    "                sentence = text_process1(sentence)\n",
    "                #tokenize and lemmatize the file\n",
    "                word_tokens = word_tokenize(sentence)\n",
    "                token_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                lemmatized_text = []\n",
    "                for word in token_text:\n",
    "                    lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "                body_sent.append(lemmatized_text)\n",
    "                sentence_list.append(lemmatized_text)\n",
    "        main_content_tokenized.append(body_sent)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "sentence_list = [s for s in sentence_list if len(s) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud\n",
    "\n",
    "freq = pd.Series(np.concatenate(sentence_list)).value_counts()\n",
    "wordcloud = WordCloud(\n",
    "            width = 300, height = 200,\n",
    "            background_color = \"white\",\n",
    "            max_words = 200,\n",
    "            max_font_size = 40,\n",
    "            stopwords = stop_words,\n",
    "            scale = 5,\n",
    "            random_state = 0).generate_from_frequencies(freq)\n",
    "fig = plt.figure(1, figsize = (9,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.savefig('WordCloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "\n",
    "#Define model\n",
    "model = Word2Vec(sentence_list, vector_size = 100, window = 8, min_count = 1, sg = 0, workers = 4)\n",
    "                               \n",
    "#Save model\n",
    "model.save(\"Word2Vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = Word2Vec.load(\"Word2Vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Take and clean query\n",
    "\n",
    "#Read input\n",
    "text = input(\"Please enter your query: \")\n",
    "text = text_process2(text)\n",
    "#tokenize and lemmatize the file\n",
    "word_tokens = word_tokenize(text)\n",
    "token_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "query = []\n",
    "for word in token_text:\n",
    "    query.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize query\n",
    "\n",
    "query_vec = [0.0 for i in range(100)]\n",
    "num = 0\n",
    "for i in query:\n",
    "    try:\n",
    "        vec = model.wv[i]\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "        query_vec += vec\n",
    "        num +=1\n",
    "if(num != 0):\n",
    "    query_vec /= num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = [] #contains top 20 of most similary vector and corresponding sentence index\n",
    "count = 0\n",
    "\n",
    "for i in range(len(sentence_list)):\n",
    "    sent_vec = [0.0 for i in range(100)]\n",
    "    num = 0\n",
    "    for j in sentence_list[i]:\n",
    "        try:\n",
    "            vec = model.wv[j]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            sent_vec += vec\n",
    "            num +=1\n",
    "    if(num != 0):\n",
    "        sent_vec /= num\n",
    "    similarity = cosine_similarity(np.expand_dims(query_vec, 0), np.expand_dims(sent_vec, 0))\n",
    "    if(count < 20):\n",
    "        sims.append(tuple((i, similarity)))\n",
    "        count += 1\n",
    "    else:\n",
    "        if(similarity > min(sims, key = itemgetter(1))[1]):\n",
    "            sims[sims.index(min(sims, key = itemgetter(1)))] = tuple((i, similarity))\n",
    "\n",
    "sims.sort(key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate most relevant sentence with source\n",
    "\n",
    "article = []\n",
    "\n",
    "print(\"Showing top 10 relevant articles\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "for n in range(len(sims)):\n",
    "    if(len(article) == 10):\n",
    "        break\n",
    "    for i in range(len(abstract_tokenized)):\n",
    "        if(i not in article):\n",
    "            for j in range(len(abstract_tokenized[i])):\n",
    "                if(abstract_tokenized[i][j] == sentence_list[sims[n][0]]):\n",
    "                    text = df_merge.loc[i]['abstract']\n",
    "                    splitted_text = text.split(sep = '.')\n",
    "                    location = text.find(splitted_text[j])\n",
    "                    s = 0\n",
    "                    e = len(text) - 1\n",
    "                    if(location - 150 > 0):\n",
    "                        s = location - 150\n",
    "                    if(location + 450 < len(text)):\n",
    "                        e = location + 450\n",
    "                    while(text[e].isalpha()):\n",
    "                        e += 1\n",
    "                    print(df_merge.loc[i]['title'] + str(i))\n",
    "                    print(\"\\n\")\n",
    "                    while(text[s].isalpha()):\n",
    "                        s += 1\n",
    "                    print(\"... \" + text[s:e] + \" ...\")\n",
    "                    if(df_merge.loc[i]['url'] != np.nan):\n",
    "                        print(\"Original document at: \" + df_merge.loc[i]['url'])\n",
    "                    else:\n",
    "                        print(\"Original document at: Not available\")\n",
    "                    article.append(i)\n",
    "                    print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "    for i in range(len(main_content_tokenized)):\n",
    "        if(i not in article):\n",
    "            for j in range(len(main_content_tokenized[i])):\n",
    "                if(main_content_tokenized[i][j] == sentence_list[sims[n][0]]):\n",
    "                    text = df_merge.loc[i]['main_content']\n",
    "                    splitted_text = text.split(sep = '.')\n",
    "                    location = text.find(splitted_text[j])\n",
    "                    s = 0\n",
    "                    e = len(text) - 1\n",
    "                    if(location - 150 > 0):\n",
    "                        s = location - 150\n",
    "                    if(location + 450 < len(text)):\n",
    "                        e = location + 450\n",
    "                    while(text[e].isalpha()):\n",
    "                        e += 1               \n",
    "                    print(df_merge.loc[i]['title'] + str(i))\n",
    "                    print(\"\\n\")\n",
    "                    while(text[s].isalpha()):\n",
    "                        s += 1\n",
    "                    print(\"... \" + text[s:e] + \" ...\")\n",
    "                    if(df_merge.loc[i]['url'] != np.nan):\n",
    "                        print(\"Original document at: \" + df_merge.loc[i]['url'])\n",
    "                    else:\n",
    "                        print(\"Original document at: Not available\")\n",
    "                    article.append(i)\n",
    "                    print(\"-------------------------------------------------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
